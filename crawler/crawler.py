import requests
from bs4 import BeautifulSoup
import pymysql
import time
from dateutil import parser

# 1. DB ì—°ê²° ì„¤ì • (ë„¤ê°€ ë§Œë“  DB ì •ë³´ë‘ ì¼ì¹˜ì‹œì¼°ì–´)
def get_db_connection():
    return pymysql.connect(
        host='localhost',
        user='root',
        password='1234',
        db='news_azit', 
        charset='utf8mb4',
        cursorclass=pymysql.cursors.DictCursor
    )

# 2. í¬ë¡¤ë§ ë° ì €ì¥ í•¨ìˆ˜
def crawl_and_save(category, search_query):
    print(f"ğŸš€ [{category}] êµ¬ê¸€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    # êµ¬ê¸€ ë‰´ìŠ¤ RSS ì£¼ì†Œ
    url = f"https://news.google.com/rss/search?q={search_query}&hl=ko&gl=KR&ceid=KR:ko"
    
    try:
        res = requests.get(url)
        # XML í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê¸° ìœ„í•´ lxml íŒŒì„œ ì‚¬ìš©
        soup = BeautifulSoup(res.text, "xml")
        items = soup.find_all("item")
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        new_count = 0
        # ìµœì‹  ë‰´ìŠ¤ 20ê°œë§Œ ê°€ì ¸ì˜¤ê¸°
        for item in items[:20]:
            title = item.title.text
            link = item.link.text
            raw_date = item.pubDate.text 
            # ë‚ ì§œ í˜•ì‹ì„ DBì— ë§ê²Œ ë³€í™˜ (YYYY-MM-DD HH:MM:SS)
            pub_date = parser.parse(raw_date).strftime('%Y-%m-%d %H:%M:%S')
            
            # ì´ë¯¸ ì €ì¥ëœ ë‰´ìŠ¤ì¸ì§€ í™•ì¸ (ì œëª©ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬)
            cursor.execute("SELECT id FROM news WHERE title = %s", (title,))
            if cursor.fetchone() is None:
                sql = "INSERT INTO news (title, link, category, provider, pub_date) VALUES (%s, %s, %s, %s, %s)"
                cursor.execute(sql, (title, link, category, "Google News", pub_date))
                new_count += 1
        
        conn.commit()
        conn.close()
        print(f"âœ… [{category}] ì €ì¥ ì™„ë£Œ: {new_count}ê±´ì˜ ìƒˆë¡œìš´ ë‰´ìŠ¤.")
    except Exception as e:
        print(f"âŒ [{category}] ì—ëŸ¬ ë°œìƒ: {e}")

# 3. ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    # ê²€ìƒ‰ì–´ ì„¤ì •
    categories = {
        "ì£¼ì‹": "ì£¼ì‹ ì¦ì‹œ", 
        "ì½”ì¸": "ë¹„íŠ¸ì½”ì¸ ê°€ìƒí™”í", 
        "ë¶€ë™ì‚°": "ë¶€ë™ì‚° ì•„íŒŒíŠ¸"
    }
    
    print("--- ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤ ---")
    for cat, query in categories.items():
        crawl_and_save(cat, query)
        time.sleep(1) # ì°¨ë‹¨ì„ ë§‰ê¸° ìœ„í•´ 1ì´ˆ ì‰¬ê¸°
    print("--- ëª¨ë“  ì‘ì—…ì´ ëë‚¬ìŠµë‹ˆë‹¤ ---")