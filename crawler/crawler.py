import requests
from bs4 import BeautifulSoup
import pymysql
import time
from dateutil import parser

# 1. DB ì—°ê²° ì„¤ì •
def get_db_connection():
    return pymysql.connect(
        host='localhost',
        user='root',
        password='1234',
        db='news_azit',
        charset='utf8mb4',
        cursorclass=pymysql.cursors.DictCursor
    )

# 2. í¬ë¡¤ë§ ë° ì €ì¥ í•¨ìˆ˜
def crawl_and_save(category, search_query):
    print(f"ğŸš€ [{category}] êµ¬ê¸€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    url = f"https://news.google.com/rss/search?q={search_query}&hl=ko&gl=KR&ceid=KR:ko"

    try:
        res = requests.get(url)
        soup = BeautifulSoup(res.text, "xml")
        items = soup.find_all("item")

        conn = get_db_connection()
        cursor = conn.cursor()

        new_count = 0

        for item in items[:20]:
            title = item.title.text
            link = item.link.text
            raw_date = item.pubDate.text
            pub_date = parser.parse(raw_date).strftime('%Y-%m-%d %H:%M:%S')

            # â˜… ìš”ì•½ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ë¹ˆì¹¸)
            description = item.description.text if item.description else ""
            # HTML íƒœê·¸ ì œê±° (ê°„ë‹¨í•˜ê²Œ)
            description = BeautifulSoup(description, "lxml").text

            # ì¤‘ë³µ í™•ì¸
            cursor.execute("SELECT id FROM news WHERE title = %s", (title,))
            if cursor.fetchone() is None:
                # DBì— ì €ì¥ (description í¬í•¨)
                sql = "INSERT INTO news (title, link, category, provider, pub_date, description) VALUES (%s, %s, %s, %s, %s, %s)"
                cursor.execute(sql, (title, link, category, "Google News", pub_date, description))
                new_count += 1

        conn.commit()
        conn.close()
        print(f"âœ… [{category}] ì €ì¥ ì™„ë£Œ: {new_count}ê±´ì˜ ìƒˆë¡œìš´ ë‰´ìŠ¤.")
    except Exception as e:
        print(f"âŒ [{category}] ì—ëŸ¬ ë°œìƒ: {e}")

# 3. ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    categories = {
        "ì£¼ì‹": "ì£¼ì‹ ì¦ì‹œ",
        "ì½”ì¸": "ë¹„íŠ¸ì½”ì¸ ê°€ìƒí™”í",
        "ë¶€ë™ì‚°": "ë¶€ë™ì‚° ì•„íŒŒíŠ¸"
    }

    print("--- ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤ ---")
    for cat, query in categories.items():
        crawl_and_save(cat, query)
        time.sleep(1)
    print("--- ëª¨ë“  ì‘ì—…ì´ ëë‚¬ìŠµë‹ˆë‹¤ ---")